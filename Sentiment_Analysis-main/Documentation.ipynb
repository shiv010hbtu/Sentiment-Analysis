{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2982e70b-8e70-4656-8004-640bc6015a03",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDB reviews\n",
    "* Import all the required modules\n",
    "* Retreive the data from the dataset\n",
    "* Visually Analyse the data by yourself and see which columns are of our use from the given dataset\n",
    "* Tokenisation of the words from the sentences\n",
    "* StopWords Removal and other punctuation removal\n",
    "* Stemming using PorterStemmer or LancasterStemmer\n",
    "* Vectorization of the model\n",
    "* 1. Binary vectorizer\n",
    "  2. Count Vectorizer\n",
    "  3. N-gram Vectorizer\n",
    "  4. TF-IDF Vectorizer\n",
    "* Split the data into training and testing data\n",
    "* Fit the data and train the model\n",
    "* Make the predictions and check it's accuracy\n",
    "* Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba337afc-1191-4d5d-9e9d-a7526ad2da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anura\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Some Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Retrieving the data from the dataset\n",
    "data = pd.read_csv('data/dataset.csv')\n",
    "data.head()\n",
    "\n",
    "# Making the features and label \n",
    "x = data['review']\n",
    "y = data['sentiment']\n",
    "# x.shape\n",
    "# y.shape\n",
    "\n",
    "# Now word tokenization and stopword removal\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize , RegexpTokenizer\n",
    "\n",
    "# Tokenizing each review using list comprehension\n",
    "x = [word_tokenize(t) for t in x]\n",
    "\n",
    "# StopWord Removal from the tokenized sentence\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "# Adding some punctuation to the sw so as to remove them too\n",
    "pun = ['.','`','~','!','@','#','$','%','^','&','*','(',')','-','_','=','+',',','<','>','.','/','?',\"'\",';',':','[',']']\n",
    "sw = sw + pun\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = [[i for i in t if i not in sw and len(i)>0] for t in x]\n",
    "\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(x[i])):\n",
    "        x[i][j] = x[i][j].lower()\n",
    "        \n",
    "    \n",
    "# We can also use RegexpTokenizer to do the same\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# for t in range(len(x)):\n",
    "#     for i in range(len(str(t))):\n",
    "#         if len(tokenizer.tokenize(x[t][i]))>0:\n",
    "#             x[t][i] = ''.join(tokenizer.tokenize(x[t][i]))\n",
    "#     x[t] = ' '.join(x[t])\n",
    "\n",
    "x = [[''.join(tokenizer.tokenize(word)) for word in t if len(tokenizer.tokenize(word))>0] for t in x]\n",
    "\n",
    "\n",
    "\n",
    "# Stemming \n",
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "\n",
    "# x = [ [porter.stem(word) for word in i] for i in x]\n",
    "x = [ [lancaster.stem(word) for word in i] for i in x]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Make the sentences now as they are in word form\n",
    "clean_data = [' '.join(t) for t in x]\n",
    "\n",
    "\n",
    "\n",
    "# Now we have to do the vectorization of the sentences\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "x = vectorizer.fit_transform(clean_data)\n",
    "\n",
    "\n",
    "\n",
    "# Converting data of sentiment into numbers\n",
    "y = y.apply( lambda x : 1 if x=='positive' else 0)\n",
    "\n",
    "\n",
    "\n",
    "# Splitting our data into testing and training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Apply the algorithm -> Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from \n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(x_train,y_train)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model2 = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# model2.fit(x_train,y_train)\n",
    "\n",
    "# model2.score(x_test,y_test)\n",
    "# model1.score(x_test,y_test)\n",
    "\n",
    "\n",
    "# Saving the model\n",
    "import pickle\n",
    "pickle.dump(model1,open('model1.pkl','wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a4d44a-b416-4812-ba74-780914329a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8984"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc459f87-5449-4600-8082-3da6b421b5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b5044-1c5e-4bf9-a3ae-5d4df9d9ea08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
